---
title: "한국어 임베딩과 R언어"
author: 홍성학
date: 2022년 9월 20일 (서울R밋업 2022.09)
output: 
  revealjs::revealjs_presentation:
    theme: night
    highlight: pygments
    center: true
    includes:
      in_header: header.html
      before_body: doc_prefix.html
      after_body: doc_suffix.html
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@100;300;400;500;700;900&display=swap');

@font-face {
    font-family: 'NEXON Lv1 Gothic OTF';
    src: url('https://cdn.jsdelivr.net/gh/projectnoonnu/noonfonts_20-04@2.1/NEXON Lv1 Gothic OTF.woff') format('woff');
    font-weight: normal;
    font-style: normal;
}

@font-face {
    font-family: 'GongGothicMedium';
    src: url('https://cdn.jsdelivr.net/gh/projectnoonnu/noonfonts_20-10@1.0/GongGothicMedium.woff') format('woff');
    font-weight: normal;
    font-style: normal;
}
@font-face {
    font-family: 'GongGothicBold';
    src: url('https://cdn.jsdelivr.net/gh/projectnoonnu/noonfonts_20-10@1.0/GongGothicBold.woff') format('woff');
    font-weight: normal;
    font-style: normal;
}
@font-face {
    font-family: 'GongGothicLight';
    src: url('https://cdn.jsdelivr.net/gh/projectnoonnu/noonfonts_20-10@1.0/GongGothicLight.woff') format('woff');
    font-weight: normal;
    font-style: normal;
}

@font-face {
    font-family: 'ONE-Mobile-Regular';
    src: url('https://cdn.jsdelivr.net/gh/projectnoonnu/noonfonts_2105_2@1.0/ONE-Mobile-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
}
@font-face {
    font-family: 'ONE-Mobile-Title';
    src: url('https://cdn.jsdelivr.net/gh/projectnoonnu/noonfonts_2105_2@1.0/ONE-Mobile-Title.woff') format('woff');
    font-weight: normal;
    font-style: normal;
}

@font-face {
    font-family: 'PyeongChang';
    src: url('https://cdn.jsdelivr.net/gh/projectnoonnu/noonfonts_2206-02@1.0/PyeongChang.woff2') format('woff2');
    font-weight: normal;
    font-style: normal;
}

@font-face {
    font-family: 'PyeongChang-Bold';
    src: url('https://cdn.jsdelivr.net/gh/projectnoonnu/noonfonts_2206-02@1.0/PyeongChang-Bold.woff2') format('woff2');
    font-weight: normal;
    font-style: normal;
}
</style>
```{css css-settings, echo=FALSE}
* {
  font-family: 'PyeongChang', 'Noto Sans KR', sans-serif;
}

.reveal h1,
.reveal h2,
.reveal h3,
.reveal h4,
.reveal h5,
.reveal h6 {
  font-family: 'GongGothicLight', 'Noto Sans KR', sans-serif;
}

.reveal .slide {
  font-family: 'ONE-Mobile-Regular', 'Nexon Lv1 Gothic OTF', 'Noto Sans KR', sans-serif;
}

.reveal .author {
  font-family: 'Nexon Lv1 Gothic OTF', 'Noto Sans KR', sans-serif;
}
.reveal .date {
  font-family: 'Nexon Lv1 Gothic OTF', 'Noto Sans KR', sans-serif;
}
```
# 내용 contents

- 한국어 자연어처리와 텍스트마이닝의 어려운점
- 자연어처리의 새 패러다임 워드임베딩

과거의 한국어 자연어처리와 텍스트마이닝과 현재 워드임베딩을 중심으로 바뀐 패러다임에서 R로 하는 자연어처리 및 텍스트마이닝

# 한국어 자연어처리 - 과거

과거 3가지 문제가 있었음

- 글자의 문제 (인코딩의 문제)
- 한국어 언어구조의 문제
- 연구를 위한 지원과 자료 부족의 문제

# 한국어 자연어처리 - 현재

**지금은**

- 글자의 문제 (인코딩의 문제) ➡️ UTF-8로 해결됨
- 한국어 언어구조의 문제 ➡️ 포기
- 연구를 위한 지원과 자료 부족의 문제 ➡️ 나아졌지만 미국에 비하면 턱없이 부족

# 한국어 자연어처리 - R로 하는 자연어처리?

- R은 텍스트 데이터를 처리하는데 매우 불리
- Python은 좀 낫지만 불리한 것은 여전
- 다른 랭귀지도 그만그만
- 컴퓨터랭귀지 문제가 아니라 텍스트와 언어의 문제가 큼

# 한국어 자연어처리 - 형태소분석기

한국어에는 쓸만한 형태소분석기, 구문분석기가 많지 않음

- 은전한닢(MeCab), 노리(Nori), 꼬꼬마(Kkma), 한나눔(Hannanum)
  - 기계학습(HMM, CRF) 기반
  - 21세기세종계획 + 각자의 추가 말뭉치
- 규칙기반(rule base)의 형태소분석기는 현재는 많이 쓰이지 않음
  - 빠르고 매우 깔금한 결과가 도출됨
  - 룰을 고치기가 매우 어려움
  - 신조어, 문법파괴, 예외사항에 취약

# 한국어 자연어처리 - 구문분석기

구문분석기 (Language Parser)는 문장을 입력하면 **트리구조**의 결과물이 나옴

"아버지가 가방을 고치신다"  
"아버지가" ➡️ 주어 / "가방을" ➡️ 목적어 / "고치신다" ➡️ 서술어

<img src="google-language-parser.png" width="800" height="450">

# 한국어 자연어처리 - 개체명인식기

**NER(Named Entity Recognition)**

```text
기생충은 스토리 만으로도 훌륭하고 일상 코메디 물과 어찌보면 스릴러가 함께 잘 혼합된 영화라고 볼 수 있습니다. 
그러나 이야기를 관통하고 많은 장면에서 보이는 계단을 통해 사회적인 높낮이와 실질적인 장소로서의 높낮이로 이야기를 풀어가고 있습니다. 
반지하, 즉 지하보다는 높지만 지상보다는 낮은 곳에 살고 있는 송강호 가족. -> 하층민 이기도 하나, 계급 이동이 가능하기도, 혹은 나락으로 떨어질 수도 있는 위치의 사람들
하루에 몇번은 빛이 듭니다.
```

<img src="ner.png" width="800" height="350">

[출처](https://github.com/songys/entity)

# 한국어 자연어처리 - 띄어쓰기 문제 #1

<center>**형태소분석기는 세그멘테이션은 할 수 있음**</center>

`아버지가방에들어가신다` ➡️ `아버지가 방에 들어가신다`

<center>**형태소분석기는 분리된 것을 결합하지는 않음**</center>

`아버지 가방을 고쳐드렸다` ➡️ `아버지가방을 고쳐드렸다`

- 처리하려면 별도의 구현이 필요하지만 매우 복잡하고 어려운 문제
- 사용자사전으로는 세그먼테이션만 방지만 가능

# 한국어 자연어처리 - 띄어쓰기 문제 #2

형태소분석기의 세그멘테이션과 띄어쓰기는 다른 것

- 형태소분석: `무궁화_꽃_이_피었_습니다`.
- 띄어쓰기: `무궁화꽃이_피었습니다`.

# 한국어 자연어처리의 장벽 - 복합명사 #1

**개체명(Named Entity)**

- 이상한 변호사 우영우
- 오징어게임
- 탑건2 매버릭
- 카라마조프가의 형제들
- 추락하는 것은 날개가 있다

**문제**

- 책이나 영화제목 등의 문장형 이름의 어려움
- 영어에서도 처리하기 어렵기는 마찬가지

# 한국어 자연어처리의 장벽 - 복합명사 #2

- 대한민국은 민주주의 공회국이다.
- 한국R사용자커뮤니티는 R을 사용하는 사람들의 커뮤니티이다.

# 한국어 자연어처리의 장벽 - 적은 연구 자료

적은 Corpus(코퍼스)로 기계학습, 전산언어연구를 하기 위한 자료와 지원이 매우 부실했음

- 모두의 말뭉치(21세기세종계획)
- 위키피디아
- 빅테크 회사들의 기여 자료
- 그외의 자료들 (크롤링, 기업체 자체 수집 후 공개)

# 자연어처리 결과의 응용

- 단어가방 기반의 기계학습, 마이닝 알고리즘 적용
  - 워드클라우드, 연관 단어 추출, 공빈발단어추출
- 문서 군집화, 분류
  - 중복문서제거, 토픽모델링, 표절방지, 자동문서분류

# 자연어처리 결과의 응용 - 워드클라우드

워드클라우드 wordcloud

- 출현한 단어의 빈도, 상대빈도로 텍스트의 크기를 다르게 시각화한 것
- 무수히 많은 문서에서 공통으로 나오는 단어는 중요한 단어로 간주
  - 단어의 중요도는 빈도가 아닌 다른 값을 사용할 수도 있음
- 가장 기본적인 빅데이터의 예제

<img src="wordcloud.png" width="800" height="450">


# 자연어처리 결과의 응용 - 워드클라우드 문제 #1

워드클라우드의 문제

- 형태소분석기에 의존적
  - 한국어는 조사와 접미사를 제거하고 명사만 뽑아내는 작업이 필요
- 불용어처리가 필요
  - 조사, 접미사, 불용어 등은 문장으로 묶어주지 않으면 의미 해석에 방해

# 자연어처리 결과의 응용 - 워드클라우드 문제 #2

**자원소모가 많고 수고가 많이 드는 작업**

- 대량의 문서에서 단어의 빈도를 추출하는 것은 분산컴퓨팅이 필요
  - 적은 양의 문서에서 추출한 단어의 빈도로는 인사이트가 보이지 않음
  - 또는 편향된 인사이트를 얻음 (특정 커뮤니티, 특정 성향의 뉴스 매체 댓글)
- R, Python, C++, Java 어떤 언어를 사용해도 노트북에서 처리하기는 어려움
- 단어의 빈도를 추출하는 것은 분산컴퓨팅 플랫폼을 사용하고 분석과 시각화는 따로 작업

# 자연어처리 결과의 응용 - 워드클라우드 문제 #3

**꿈보다는 해몽**

- 해석하는 사람의 역랑에 따라 인사이트가 달라짐
  - 자신에게 유리한 쪽으로 해석
  - 보고 싶은 것만 본다
- 좋은 인사이트를 얻기 위해서는 도메인지식이 필요함
- 도메인에 따른 주변지식도 필요함

<img src="good-insight.jpg" width="800" height="450">

# 자연어처리 결과의 응용 - 워드클라우드 문제 #4

- 생성은 자동화가 가능함
- 해석은 자동화가 안됨

# 자연어처리 결과의 응용 - 클러스터링

자연어처리를 위해서 유사문서, 중복문서를 묶는데 클러스터링을 사용

- 최근 어떤 문서(UGC)들이 주로 생성되는가?
  - 뉴스, 블로그 콘텐츠, SMS, 댓글
- 최근 어떤 문서들이 많이 소비되는가?
  - 글 조회수, 좋아요 수, 댓글 수, 공유 수

# 자연어처리 결과의 응용 - 클러스터링 문제 #1

- 문서의 볼륨이 클수록 많은 컴퓨터자원이 필요함
- 결과물을 응용해서 비즈니스에 적용하기 어려움
  - 가치화 문제 (Monetization)

# 자연어처리 결과의 응용 - 클러스터링 문제 #2

- 생각보다 잘 되지 않는 클러스터링
- 의미중심의 클러스터링이 아닌 단어매치 기반에 의한 클러스터링
  - TF-IDF와 코사인유사도(Cosine Similarity)만으로는 어려움

# 자연어처리 결과의 응용 - 학습데이터의 부족

- 자언어처리 모델을 만드는데 필요한 코퍼스가 부족
- 각 산업별, 기업별 자료들이 공개되지 않고 폐쇠적으로 활용
- 수집된 오픈데이터(크롤)도 공개가 되지 않음
  - 저작권 문제?
# 지금까지의 자연어처리의 문제

- 온톨로지의 실패
- 형태소분석기와 구문분석기의 보편화 실패
    - 빅테크 회사내에서만 독점적 관리
# 언어를 인코딩하는 2가지 기본 방법

- TF-IDF scheme
- 원핫(One hot encoding)

# 언어를 인코딩하는 2가지 기본 방법 - TFIDF

Term Frequency Inverse Document Frequency 

- TF: 토큰의 갯수
- DF: 토큰의 출현한 고유 문서 수
- 공식: TF x IDF
- 정보검색에 적합함

# 언어를 인코딩하는 2가지 기본 방법 - TFIDF

<img src="tfidf_sketch.png" width="800" height="450">

그림출처: [TFIDF sketch](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)

# TFIDF - R패키지 tidytext

```r
ibrary(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)
# ...
# 이하 생략. 코드가 조금 길지만 명료함
# ...
```

[tidytext](https://www.tidytextmining.com/tfidf.html)

# 언어를 인코딩하는 2가지 기본 방법 - TFIDF (superml 패키지)

```r
library(superml)
sents <-  c('i am going home and home',
          'where are you going.? //// ',
          'how does it work',
          'transform your work and go work again',
          'home is where you go from to work')
sents <- rep(sents, 10) 
tfv <- TfIdfVectorizer$new(max_features = 10, remove_stopwords = FALSE)
tf_mat <- tfv$fit_transform(sents)
head(tf_mat, 3)
#>      work      home       you     where     going go       and your transform
#> [1,]    0 0.8164966 0.0000000 0.0000000 0.4082483  0 0.4082483    0         0
#> [2,]    0 0.0000000 0.5773503 0.5773503 0.5773503  0 0.0000000    0         0
#> [3,]    1 0.0000000 0.0000000 0.0000000 0.0000000  0 0.0000000    0         0
#>      to
#> [1,]  0
#> [2,]  0
#> [3,]  0
```

[superml](https://cran.r-project.org/web/packages/superml/vignettes/Guide-to-TfidfVectorizer.html)


# 언어를 인코딩하는 2가지 기본 방법 - 원핫인코딩

원핫인코딩 (One hot encoding)

**"무궁화 꽃이 피었습니다"**

| 단어 | 인덱스 | 원핫벡터 |
|------|--------|----------|
| 무궁화 |   0    |  1 0 0 |
| 꽃이   |   1    |  0 1 0 |
| 피었습니다 | 2 |  0 0 1 |

# 워드임베딩

- 단어를 밀도가 높은 벡터로 표현하는 방법
  - 인코딩한 고차원의 워드벡터를 낮은 차원의 벡터로 축소하는 방법
- 지정한 윈도우 내의 모든 단어(토큰)을 원핫벡터로 바꿈

원문: "자율주행 자동차는 기계가 스스로 운행하는 자동차를 말한다."

# TF-IDF와 word2vec 계열의 차이

- TF-IDF: Bag of Words (단어가방), 주변단어를 고려하지 않음
  - "속도 위반", "위반 속도"
- word2vec:
  - 주변 단어를 고려함

# 워드임베딩 word embedding

워드임베잉 Word Embedding이란?

- 단어를 밀도높은 수열(숫자)로 만드는 기술
  - 원핫 인코딩을 하고 차원축소로 임베딩을 함
  - 단어를 의미를 잘 내포하는 벡터로 만드는 기술
- 딥러닝(Deep Learning)을 이용한 자언어처리에서는 현재 주류(대세)
  - 원핫인코딩을 사용하지 않을 수도 있음

# 워드임베딩의 장점

- 대량의 학습데이터로 높은 성능의 모델을 쉽게 만들 수 있음
- 형태적 매치 보다는 의미 매치에 가까운 모델을 만달 수 있음
- 모델을 빌드하는데 인간의 고민이 덜 필요함
- 사전학습모델(pre-trained model)을 이용해서 전이학습(transfer learning) 가능

# 워드임베딩의 단점

- 많은 컴퓨터 계산 자원이 필요함 (특히 GPU)
- 많은 학습데이터가 필요함 (적으면 성능이 낮아짐)
- 블랙박스(Blackbox Model)
  - 의도한 대로 만들어지지 않을 수 있음
  - 예상 밖의 결과가 나오는 경우가 많음

# 워드임베딩의 전이학습



# 알려진 워드임베딩 기법

- Word2Vec
- GLoVe
- ELMo
- FastText
- BERT

# 워드임베딩 - word2vec

- 2013년 구글 리서팀이 발표
- 신경망 모델 (Neural Network Model)
- 딥러닝 (Deep learning) (x)

# 워드임베딩 - GLoVE

- GLoVe (Global Vectors for Word Representation)
- Word2Vec, LSA(Latent Semantic Analysis)의 단점을 보완
- 동시등장행렬 (Co-oocurrence Matrix)을 사용
- 고차원 행렬연산(Matrix Factorization)이 부담

# 워드임베딩 - GLoVE R패키지

[text2vec](https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html#Word_embeddings)

```r
library(text2vec)
text8_file = "~/text8"
if (!file.exists(text8_file)) {
  download.file("http://mattmahoney.net/dc/text8.zip", "~/text8.zip")
  unzip ("~/text8.zip", files = "text8", exdir = "~/")
}
wiki = readLines(text8_file, n = 1, warn = FALSE)
tokens <- space_tokenizer(wiki)
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min = 5L)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
glove <- GlobalVectors$new(rank = 50, x_max = 10)
wv_main <- glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
wv_context <- glove$components
word_vectors <- wv_main + t(wv_context)
```

# 워드임베딩 - fastText

- 페이스북 리서치에서 만든 서브워드를 지원하는 임베딩 모델
- 서브워드 저연어에 의존성이 매우 적음
- GPU가 없어도 매우 빠름

# 워드임베딩 - fastText R패키지

[fastText](https://github.com/mlampros/fastText)

# 워드임베딩 - BERT

- 트랜스포머의 단순화 버전
- 현재까지 의미벡터를 만드는 방법 중 가장 좋은 성능을 보임
  - SOTA (State of the Art)

# Word2vec 구조

그림 필요

# BERT

- BERT(Bidirectional Encoder Representations from Transformers)
- BERT는 토큰임베딩, 세그먼트임베딩, 포지션임베딩으로 구성되어 있음

# BERT - 토큰임베딩 Token Embeddings

- 워드피쓰(Word piece) 임베딩 방식을 사용
  - 자주 등장하는 가장 긴 길이의 sub-word을 하나의 단위로 사용
  - 자주 등장하는 단어(sub-word)는 그 자체가 단위가 되고, 자주 등장하지 않는 단어(rare word)는 더 작은 sub-word로 분할
  - 자주 등장하지 않은 단어가 전부 Out-of-vocabulary(OOV)가 되는 성능 저하의 원인을 해결.
  - 입력 문장의 시작과 끝으로 [CLS], [SEP] 토큰을 지정.
    - [CLS] (special classification token) 토큰은 시퀀스 결합의 의미
    - 또한 문장의 구분을 위해 문장의 끝에 [SEP] 토큰을 사용 합니다.

# BERT - 세그먼트임베딩 Segment Embeddings

- 토큰으로 나누어진 단어들을 다시 하나의 문장으로 만듦
- 첫 번째 [SEP] 토큰까지는 0, 그 외 [SEP] 토큰은 1로 마스킹해서 문장을 구별

# BERT - 포지션임베딩 Position Embeddings

- 토큰의 순서를 인코딩
- BERT는 transformer의 encoder를 사용하는데 Transformer는 Self-Attention 모델을 사용
- Self-Attention은 입력의 위치에 대해 고려하지 못하므로 입력 토큰의 위치 정보 주어야 함
- Transformer 에서는 Sigsoid 함수를 이용한 Positional encoding을 사용하지만
- BERT에서는 이를 변형하여 Position Encodings을 사용
- 앞서 언급한 각 임베딩들의 토큰 별로 모두 더하여 BERT의 입력 벡터로 사용합니다.

# 워드벡터의 활용

여전히 과거의 방법과 동일한 방법을 시도

- 분류(Classification)
  - 신경망(Neural Network), SVM, Knn
- 군집화(Clutering)
  - K-means, DBSCAN

※ 예전에도 하던 것들이지만 더 그럴듯한 쓸만한 결과가 나옴

# 워드벡터의 활용

- 의미기반의 문서 검색
- 유사한 상품명 찾기, 의미 기반의 문서 분류
- 감성분류

# 워드벡터 활용을 위한 유용한 도구

- 고속벡터검색엔진
  - Milvus, ElasticSearch, Vespa
  - Faiss

[awesome vector search 참조](https://github.com/currentslab/awesome-vector-search)


# R언어로 자연어처리를 할 때? - 어떻게 접근할까?

- 워드임베딩을 사용해야 하나? TFIDF벡터라이즈를 사용해야 하나?

# R언어로 워드임베딩을 할 때 고려할 것 #1

- 어떤 모델을 사용할까?
  - 문제를 해결하는데 가장 적합한 솔루션을 사용
  - 가장 좋은 솔루션 보다는 가장 효율적인 솔루션
  - 좋은 것을 원하면 그냥 BERT를 쓰면 됨
- 입력 데이터 특징과 양상에 대한 확인
  - 장문인가? 단문인가? 초단문인가?
  - 문법파괴, 언어파괴가 심한가 아닌가?
    - 뉴스 매체테의 텍스트는 문법파괴와 맞춤법 오류가 거의 없음
    - SNS, 채팅 텍스트는 문법파괴와 맞춤법 오류가 많음

# R언어로 워드임베딩을 할 때 고려할 것 #2

- 모델을 빌드부터 할 것인가?
  - R에서 빌드가 가능한 모델 구현체 패키지를 입수하거나 만들어야함
  - 꼭 그럴 필요가 있는지 고민
  - 최근 많은 기계학습 프레임워크는 모델 빌더를 제공함
- 빌드된 모델로 결과를 해석하고 응용하는 것만 할 것인가?
  - 빌드된 모델을 로딩해서 사용가능한 패키지가 있어야 함
  - 일부는 있고 일부는 없음
- 응용한 결과로 처리된 데이터로 인사이트를 얻는 것만 할 것인가?
  - 기존의 데이터분석, 데이터과학과 다를 것은 없음

# 참고자료

- https://medium.com/broadhorizon-cmotions/nlp-with-r-part-2-training-word-embedding-models-and-visualize-results-ae444043e234
- https://rpubs.com/nabiilahardini/word2vec
- https://cran.r-project.org/web/packages/word2vec/readme/README.html
- https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html
