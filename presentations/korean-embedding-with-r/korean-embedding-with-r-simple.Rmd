---
title: "한국어 임베딩과 R언어"
subtitle: "자연어처리의 새로운 패러다임"
author: "홍성학"
date: "2022년 9월 24일"
output: 
  revealjs::revealjs_presentation:
    theme: "night"
    highlight: "pygments"
    center: true
    transition: "slide"
    self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@100;300;400;500;700;900&display=swap');

/* 기본 폰트 설정 */
.reveal {
  font-family: 'Noto Sans KR', sans-serif !important;
  font-size: 32px !important;
  font-weight: normal;
  color: #fff !important;
}

.reveal h1,
.reveal h2,
.reveal h3,
.reveal h4,
.reveal h5,
.reveal h6 {
  font-family: 'Noto Sans KR', sans-serif !important;
  color: #fff !important;
  line-height: 1.2;
  letter-spacing: -0.03em;
  text-transform: none;
  text-shadow: none;
  word-wrap: break-word;
  margin: 0.5em 0 !important;
}

.reveal h1 {
  font-size: 2.2em !important;
  font-weight: 700 !important;
}

.reveal h2 {
  font-size: 1.6em !important;
  font-weight: 600 !important;
}

.reveal h3 {
  font-size: 1.3em !important;
  font-weight: 500 !important;
}

.reveal p {
  margin: 15px 0 !important;
  line-height: 1.6 !important;
}

.reveal ul, .reveal ol {
  margin: 15px 0 !important;
  line-height: 1.6 !important;
}

.reveal li {
  margin: 8px 0 !important;
}

.reveal blockquote {
  font-style: italic;
  background: rgba(255, 255, 255, 0.1) !important;
  border-left: 5px solid #42affa !important;
  padding: 15px !important;
  margin: 15px 0 !important;
}

.reveal code {
  font-family: 'Consolas', 'Monaco', monospace !important;
  background: rgba(255, 255, 255, 0.1) !important;
  padding: 2px 6px !important;
  border-radius: 3px !important;
  font-size: 0.85em !important;
}

.reveal pre {
  background: rgba(0, 0, 0, 0.3) !important;
  border-radius: 5px !important;
  padding: 15px !important;
  margin: 15px 0 !important;
  overflow: auto !important;
  font-size: 0.75em !important;
}

.reveal pre code {
  background: none !important;
  padding: 0 !important;
  border-radius: 0 !important;
  max-height: 400px !important;
  word-wrap: normal !important;
}

.reveal table {
  margin: 15px auto !important;
  border-collapse: collapse !important;
  border-spacing: 0 !important;
  font-size: 0.8em !important;
}

.reveal table th,
.reveal table td {
  text-align: left !important;
  padding: 0.3em 0.6em !important;
  border-bottom: 1px solid rgba(255, 255, 255, 0.3) !important;
}

.reveal table th {
  font-weight: bold !important;
  background: rgba(255, 255, 255, 0.1) !important;
}

.reveal img {
  margin: 15px auto !important;
  max-width: 90% !important;
  height: auto !important;
  border: none !important;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.3) !important;
  display: block !important;
}

/* 강조 스타일 */
.highlight {
  background: rgba(66, 175, 250, 0.3) !important;
  padding: 3px 8px !important;
  border-radius: 3px !important;
  color: #42affa !important;
  font-weight: bold !important;
}

.important {
  color: #ff6b6b !important;
  font-weight: bold !important;
}

.success {
  color: #51cf66 !important;
  font-weight: bold !important;
}

/* 중앙 정렬 */
.center {
  text-align: center !important;
}

/* 작은 텍스트 */
.small {
  font-size: 0.8em !important;
}

/* 큰 텍스트 */
.large {
  font-size: 1.2em !important;
}

/* 슬라이드 레이아웃 개선 */
.reveal .slides section {
  text-align: left !important;
  padding: 20px !important;
}

.reveal .slides section.center {
  text-align: center !important;
}

.reveal .slides section .center {
  text-align: center !important;
}
</style>

# 한국어 임베딩과 R언어

<div class="center">
**자연어처리의 새로운 패러다임**

과거의 한국어 자연어처리 한계를 극복하고  
워드임베딩을 중심으로 한 현대적 접근법과  
R에서 활용 가능한 도구들을 소개합니다
</div>

---

## 목차 {.center}

1. **한국어 자연어처리의 역사와 한계**
2. **워드임베딩: 새로운 패러다임**
3. **주요 임베딩 기법들**
4. **R에서의 구현과 활용**
5. **실제 응용 사례**
6. **미래 전망**

---

## 한국어 자연어처리의 과거 문제점

### 3가지 주요 장벽

1. **<span class="important">글자 인코딩 문제</span>**
   - EUC-KR, CP949 등 다양한 인코딩
   - 문자 깨짐 현상 (mojibake)

2. **<span class="important">언어 구조적 특성</span>**
   - 교착어적 특성 (조사, 어미 변화)
   - 복잡한 형태소 구조

3. **<span class="important">리소스 부족</span>**
   - 말뭉치 데이터 부족
   - 연구 지원 및 도구 부족

---

## 현재 상황: 개선과 과제

### ✅ 해결된 문제들
- **인코딩 문제**: UTF-8 표준화로 해결
- **도구 발전**: 다양한 형태소 분석기 등장

### ⚠️ 여전한 과제들
- **언어 구조적 복잡성**: 여전히 어려움
- **리소스 격차**: 영어권 대비 부족
- **도메인별 특화**: 분야별 맞춤형 도구 부족

---

## 한국어 형태소 분석기 생태계

### 주요 도구들

- **<span class="highlight">은전한닢(MeCab)</span>**: 일본어 기반, 한국어 적용
- **<span class="highlight">노리(Nori)</span>**: Elasticsearch 내장
- **<span class="highlight">꼬꼬마(Kkma)</span>**: 서울대 개발
- **<span class="highlight">한나눔(Hannanum)</span>**: KAIST 개발
- **<span class="highlight">OKT</span>**: 오픈소스 한국어 처리기
- **<span class="highlight">Soynlp</span>**: 비지도 학습 기반
- **<span class="highlight">SentencePiece</span>**: 구글 개발

### 기술적 접근
- **기계학습 기반**: HMM, CRF 모델 활용
- **말뭉치**: 21세기 세종계획 + 추가 데이터

---

## 워드임베딩: 패러다임의 전환

### 기존 방식의 한계
- **희소 표현**: 대부분 0인 고차원 벡터
- **의미 부재**: 단어 간 의미적 관계 파악 불가
- **문맥 무시**: 주변 단어 정보 활용 불가

### 워드임베딩의 혁신
- **밀집 표현**: 낮은 차원의 실수 벡터
- **의미 보존**: 유사한 의미의 단어는 유사한 벡터
- **문맥 활용**: 주변 단어를 고려한 학습

---

## 주요 워드임베딩 기법들

### 1세대: 정적 임베딩
- **<span class="highlight">Word2Vec</span>** (2013, Google)
- **<span class="highlight">GloVe</span>** (2014, Stanford)
- **<span class="highlight">FastText</span>** (2016, Facebook)

### 2세대: 문맥적 임베딩
- **<span class="highlight">ELMo</span>** (2018, AllenNLP)
- **<span class="highlight">BERT</span>** (2018, Google)
- **<span class="highlight">GPT</span>** (2018, OpenAI)

---

## Word2Vec: 신경망 기반 임베딩

### 두 가지 아키텍처

**CBOW (Continuous Bag of Words)**
- 주변 단어로 중심 단어 예측
- 작은 데이터셋에 적합

**Skip-gram**
- 중심 단어로 주변 단어 예측
- 큰 데이터셋에 적합

---

## R에서 Word2Vec 구현

### wordVectors 패키지

```r
library(wordVectors)

# 모델 훈련
model <- train_word2vec(
  train_file = "corpus.txt",
  output_file = "word2vec_model.bin",
  vectors = 100,        # 벡터 차원
  threads = 4,          # 스레드 수
  window = 5,           # 윈도우 크기
  iter = 5              # 반복 횟수
)

# 모델 로드 및 유사 단어 검색
model <- read.binary.vectors("word2vec_model.bin")
similar_words <- nearest_to(model, model[["마케팅"]], 10)
```

---

## FastText: 서브워드 임베딩

### 혁신적 특징
- **서브워드 활용**: 단어 내부 구조 고려
- **OOV 문제 해결**: 미등록 단어 처리 가능
- **언어 독립적**: 다양한 언어에 적용 가능
- **빠른 속도**: GPU 없이도 고속 처리

### 한국어 처리 장점
- 복합어 처리 우수
- 신조어 대응 가능
- 형태소 변화 고려

---

## BERT: 트랜스포머 기반 임베딩

### 핵심 혁신
- **양방향 인코딩**: 좌우 문맥 모두 고려
- **트랜스포머 아키텍처**: 어텐션 메커니즘 활용
- **사전 훈련**: 대량 데이터로 사전 학습
- **파인튜닝**: 특정 작업에 맞춤 조정

### 구성 요소
- **토큰 임베딩**: WordPiece 기반
- **세그먼트 임베딩**: 문장 구분
- **위치 임베딩**: 순서 정보

---

## 실제 응용 사례

### 1. 의미적 검색 (Semantic Search)
- 키워드 매칭 → 의미 매칭
- 유사 문서 검색
- 추천 시스템

### 2. 문서 분류 (Document Classification)
- 감성 분석
- 주제 분류
- 스팸 필터링

### 3. 기계 번역 (Machine Translation)
- 신경망 기계 번역
- 다국어 임베딩

---

## 코사인 유사도 활용

### 개념
- 벡터 간 각도 측정
- 방향성 중심의 유사도
- 크기에 무관한 유사도

### 수식
```
cosine_similarity(A, B) = (A · B) / (|A| × |B|)
```

---

## 실습: 한국어 Word2Vec

### 1. 데이터 준비

```r
library(wordVectors)
library(dplyr)

# 한국어 텍스트 데이터 로드
korean_text <- readLines("korean_corpus.txt", encoding = "UTF-8")

# 전처리
clean_text <- korean_text %>%
  # 특수문자 제거
  gsub("[^가-힣a-zA-Z0-9\\s]", "", .) %>%
  # 다중 공백 제거
  gsub("\\s+", " ", .) %>%
  # 앞뒤 공백 제거
  trimws()

# 파일로 저장
writeLines(clean_text, "preprocessed_corpus.txt")
```

---

## 실습: 모델 훈련

### 2. Word2Vec 모델 훈련

```r
# 모델 훈련
model <- train_word2vec(
  train_file = "preprocessed_corpus.txt",
  output_file = "korean_word2vec.bin",
  vectors = 200,      # 임베딩 차원
  threads = 4,        # 병렬 처리
  window = 5,         # 윈도우 크기
  iter = 5,           # 반복 횟수
  min_count = 10,     # 최소 빈도
  cbow = 0            # Skip-gram 사용
)

# 모델 로드
model <- read.binary.vectors("korean_word2vec.bin")
```

---

## 실습: 결과 분석

### 3. 유사 단어 검색

```r
# 특정 단어와 유사한 단어들 찾기
similar_words <- nearest_to(model, model[["한국어"]], 10)
print(similar_words)

# 단어 간 유사도 계산
similarity <- cosineSimilarity(model[["컴퓨터"]], model[["기계"]])
print(paste("유사도:", similarity))

# 단어 벡터 시각화
library(Rtsne)
library(ggplot2)

# 주요 단어들 선택
key_words <- c("한국어", "언어", "컴퓨터", "기계", "학습", "데이터")
word_vectors <- model[key_words, ]

# t-SNE 적용
tsne_result <- Rtsne(word_vectors, dims = 2, perplexity = 3)

# 시각화
plot_data <- data.frame(
  word = key_words,
  x = tsne_result$Y[, 1],
  y = tsne_result$Y[, 2]
)

ggplot(plot_data, aes(x = x, y = y, label = word)) +
  geom_point(size = 3) +
  geom_text(hjust = 0, vjust = 0, size = 4) +
  theme_minimal() +
  labs(title = "한국어 단어 임베딩 시각화")
```

---

## 한국어 처리 특화 전략

### 1. 형태소 분석 연계
```r
library(RcppMeCab)

# 형태소 분석 후 임베딩
morphs <- pos(text, format = "data.frame")
noun_text <- morphs %>%
  filter(pos %in% c("NNG", "NNP")) %>%
  pull(token) %>%
  paste(collapse = " ")
```

### 2. 서브워드 활용
```r
# FastText로 OOV 문제 해결
fasttext_model <- fasttext_interface(
  list(command = 'skipgram', 
       wordNgrams = 2,  # 서브워드 활용
       bucket = 2000000)
)
```

---

## 최신 동향과 미래

### 현재 트렌드
- **대규모 언어 모델**: GPT, BERT 계열
- **다국어 모델**: mBERT, XLM-R
- **효율적 모델**: DistilBERT, ALBERT

### 한국어 특화 모델
- **KoBERT**: SKT 개발
- **KoGPT**: 카카오브레인 개발
- **HanBERT**: 한국전자통신연구원

### 미래 전망
- **더 큰 모델**: 파라미터 수 증가
- **멀티모달**: 텍스트 + 이미지
- **실시간 처리**: 엣지 컴퓨팅

---

## 결론

### 핵심 메시지
1. **패러다임 전환**: TF-IDF → 워드임베딩
2. **의미 중심**: 형태적 → 의미적 유사성
3. **실용적 접근**: 문제에 맞는 도구 선택
4. **지속적 발전**: 새로운 기법 학습

### R 사용자 관점
- **강력한 도구**: 다양한 패키지 활용
- **Python 연동**: 최신 기법 접근
- **실무 적용**: 비즈니스 가치 창출

---

## 질문과 답변 {.center}

**감사합니다!**

<div class="center">
더 궁금한 점이 있으시면  
언제든지 질문해 주세요
</div> 